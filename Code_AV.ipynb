{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "import gensim\n",
    "import fastparquet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'train_data.csv'\n",
    "def load_data(url=URL):\n",
    "    return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10494</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The question about God and the Veterans. What ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I thought #LastComicStanding airs on Wednesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Bingo! Put that in your article!!! #GOPDebates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Fox is cherry picking the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Waiting on Trumps answer about God #GOPDebates...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "10494  Negative  The question about God and the Veterans. What ...\n",
       "10495  Negative  I thought #LastComicStanding airs on Wednesday...\n",
       "10496  Negative  Bingo! Put that in your article!!! #GOPDebates...\n",
       "10497  Negative  RT @RWSurferGirl: Fox is cherry picking the ca...\n",
       "10498   Neutral  Waiting on Trumps answer about God #GOPDebates..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Collecting stopwords from different sources and merging them deleting duplicates\n",
    "\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "# function to delete duplicates\n",
    "def del_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "stopword_removed_sentences = []\n",
    "sw = list(stopwords.words(\"English\"))\n",
    "\n",
    "response = requests.get('http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop')\n",
    "sw.extend(response.text.split())\n",
    "sw.extend([\"rt\", \"retweet\"])\n",
    "for i in sw:\n",
    "    sw = [ re.sub('[\\W_]+', '', x) for x in sw ]\n",
    "#sw.append((i for i in list1 if str(i) not in sw).next())\n",
    "#sw = sw.map\n",
    "sw = del_dup(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleantweet1(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(cleantweet1.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "def cleantweet2(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list of hashtags'''\n",
    "    s = re.findall(r'#(\\w+)', s)\n",
    "    s = \" \".join([word.lower() for word in s])\n",
    "    s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n",
    "def cleantweet3(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : tweet without hashtags'''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(cleantweet3.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    return s\n",
    "\n",
    "def cleantweet4(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    s = re.sub(cleantweet4.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    #s = \" \".join([word.lower() for word in s.split() if word.lower() not in sw])\n",
    "    #s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "\n",
    "\n",
    "    # tokenize tweet into sentences\n",
    "    sents = sent_tokenize(s)\n",
    "    # tokenize sentences into list of words\n",
    "    words = [word_tokenize(s) for s in sents]\n",
    "    # NO IDEA\n",
    "    words = [e for sent in words for e in sent]\n",
    "    return [cleantweet4.stemmer.stem(e.lower()) for e in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleantweet1.pattern = re.compile(r'(@\\w+)|[^a-zA-Z\\s]', flags=re.IGNORECASE)\n",
    "cleantweet3.pattern = re.compile(r'(@\\w+)|(#\\w+)|[^a-zA-Z\\s]|(\\w+:\\/\\/\\S+)', flags=re.IGNORECASE)\n",
    "#start = time.perf_counter()\n",
    "#df[\"text_clean1\"] = df.iloc[0:].text.apply(cleantweet1)\n",
    "#following is consistently faster\n",
    "df.loc[:,'text_clean1'] = df.loc[:,'text'].map(cleantweet1)\n",
    "#end = time.perf_counter()\n",
    "df.loc[:,'hashtags'] = df.loc[:,'text'].map(cleantweet2)\n",
    "#print(end - start)\n",
    "# Removing the morphological and inflexional endings from words in English.\n",
    "#cleantweet4.stemmer = PorterStemmer()\n",
    "df.loc[:,'wordlist'] = df.loc[:,'text'].map(cleantweet3)\n",
    "df.loc[:,'no_names_hashtags'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))\n",
    "#df.loc[:,'wordstring'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>no_names_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>feel climate change question night gopdebate</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[feel, climate, change, question, night]</td>\n",
       "      <td>feel climate change question night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>catch full gopdebate night scotts lines second...</td>\n",
       "      <td>[gopdebate, walker]</td>\n",
       "      <td>[catch, full, night, scotts, lines, seconds]</td>\n",
       "      <td>catch full night scotts lines seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>mention tamir rice gopdebate held cleveland wow</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[mention, tamir, rice, held, cleveland, wow]</td>\n",
       "      <td>mention tamir rice held cleveland wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>carly fiorina trending hours debate men justco...</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[carly, fiorina, trending, hours, debate, men,...</td>\n",
       "      <td>carly fiorina trending hours debate men justco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>gopdebate delivered highest ratings history pr...</td>\n",
       "      <td>[gopdebate, trump]</td>\n",
       "      <td>[delivered, highest, ratings, history, preside...</td>\n",
       "      <td>delivered highest ratings history presidential...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...   \n",
       "1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...   \n",
       "2   Neutral  RT @TJMShow: No mention of Tamir Rice and the ...   \n",
       "3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...   \n",
       "4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...   \n",
       "\n",
       "                                         text_clean1             hashtags  \\\n",
       "0       feel climate change question night gopdebate          [gopdebate]   \n",
       "1  catch full gopdebate night scotts lines second...  [gopdebate, walker]   \n",
       "2    mention tamir rice gopdebate held cleveland wow          [gopdebate]   \n",
       "3  carly fiorina trending hours debate men justco...          [gopdebate]   \n",
       "4  gopdebate delivered highest ratings history pr...   [gopdebate, trump]   \n",
       "\n",
       "                                            wordlist  \\\n",
       "0           [feel, climate, change, question, night]   \n",
       "1       [catch, full, night, scotts, lines, seconds]   \n",
       "2       [mention, tamir, rice, held, cleveland, wow]   \n",
       "3  [carly, fiorina, trending, hours, debate, men,...   \n",
       "4  [delivered, highest, ratings, history, preside...   \n",
       "\n",
       "                                   no_names_hashtags  \n",
       "0                 feel climate change question night  \n",
       "1              catch full night scotts lines seconds  \n",
       "2              mention tamir rice held cleveland wow  \n",
       "3  carly fiorina trending hours debate men justco...  \n",
       "4  delivered highest ratings history presidential...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write data, SNAPPY not available on win10 ?\n",
    "import fastparquet\n",
    "fastparquet.write('5col_DFrame.parq', df, compression='GZIP')\n",
    "#df.to_csv('5col_DFrame.csv') file size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###FAST TEXT###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
