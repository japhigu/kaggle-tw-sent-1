{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldaw\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\goldaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goldaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#from gensim.models.wrappers import FastText\n",
    "import gensim\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'train_data.csv'\n",
    "def load_data(url=URL):\n",
    "    return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10494</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The question about God and the Veterans. What ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I thought #LastComicStanding airs on Wednesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Bingo! Put that in your article!!! #GOPDebates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Fox is cherry picking the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Waiting on Trumps answer about God #GOPDebates...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "10494  Negative  The question about God and the Veterans. What ...\n",
       "10495  Negative  I thought #LastComicStanding airs on Wednesday...\n",
       "10496  Negative  Bingo! Put that in your article!!! #GOPDebates...\n",
       "10497  Negative  RT @RWSurferGirl: Fox is cherry picking the ca...\n",
       "10498   Neutral  Waiting on Trumps answer about God #GOPDebates..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\goldaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Collecting stopwords from different sources and merging them deleting duplicates\n",
    "\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "# function to delete duplicates\n",
    "def del_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "stopword_removed_sentences = []\n",
    "sw = list(stopwords.words(\"English\"))\n",
    "\n",
    "response = requests.get('http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop')\n",
    "sw.extend(response.text.split())\n",
    "sw.extend([\"rt\", \"retweet\"])\n",
    "for i in sw:\n",
    "    sw = [ re.sub('[\\W_]+', '', x) for x in sw ]\n",
    "#sw.append((i for i in list1 if str(i) not in sw).next())\n",
    "#sw = sw.map\n",
    "sw = del_dup(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleantweet1(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(r\"http\\S+\", '', s)\n",
    "    s = re.sub(r\"t.co\\s+\", '', s)\n",
    "    s = re.sub(cleantweet1.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "def cleantweet2(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list of hashtags'''\n",
    "    s = re.findall(r'#(\\w+)', s)\n",
    "    s = \" \".join([word.lower() for word in s])\n",
    "    s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n",
    "def cleantweet3(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : tweet without hashtags'''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(cleantweet3.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    return s\n",
    "\n",
    "def cleantweet4(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    s = re.sub(cleantweet4.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    #s = \" \".join([word.lower() for word in s.split() if word.lower() not in sw])\n",
    "    #s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "\n",
    "\n",
    "    # tokenize tweet into sentences\n",
    "    sents = sent_tokenize(s)\n",
    "    # tokenize sentences into list of words\n",
    "    words = [word_tokenize(s) for s in sents]\n",
    "    # NO IDEA\n",
    "    words = [e for sent in words for e in sent]\n",
    "    return [cleantweet4.stemmer.stem(e.lower()) for e in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleantweet1.pattern = re.compile(r'(@\\w+)|[^a-zA-Z\\s]', flags=re.IGNORECASE)\n",
    "\n",
    "cleantweet3.pattern = re.compile(r'(@\\w+)|(#\\w+)|[^a-zA-Z\\s]|(\\w+:\\/\\/\\S+)', flags=re.IGNORECASE)\n",
    "#start = time.perf_counter()\n",
    "#df[\"text_clean1\"] = df.iloc[0:].text.apply(cleantweet1)\n",
    "#following is consistently faster\n",
    "df.loc[:,'text_clean1'] = df.loc[:,'text'].map(cleantweet1)\n",
    "#end = time.perf_counter()\n",
    "df.loc[:,'hashtags'] = df.loc[:,'text'].map(cleantweet2)\n",
    "#print(end - start)\n",
    "# Removing the morphological and inflexional endings from words in English.\n",
    "#cleantweet4.stemmer = PorterStemmer()\n",
    "df.loc[:,'wordlist'] = df.loc[:,'text'].map(cleantweet3)\n",
    "df.loc[:,'no_names_hashtags'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))\n",
    "#df.loc[:,'wordstring'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>no_names_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>feel climate change question night gopdebate</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[feel, climate, change, question, night]</td>\n",
       "      <td>feel climate change question night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>catch full gopdebate night scotts lines second...</td>\n",
       "      <td>[gopdebate, walker]</td>\n",
       "      <td>[catch, full, night, scotts, lines, seconds]</td>\n",
       "      <td>catch full night scotts lines seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>mention tamir rice gopdebate held cleveland wow</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[mention, tamir, rice, held, cleveland, wow]</td>\n",
       "      <td>mention tamir rice held cleveland wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>carly fiorina trending hours debate men justco...</td>\n",
       "      <td>[gopdebate]</td>\n",
       "      <td>[carly, fiorina, trending, hours, debate, men,...</td>\n",
       "      <td>carly fiorina trending hours debate men justco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>gopdebate delivered highest ratings history pr...</td>\n",
       "      <td>[gopdebate, trump]</td>\n",
       "      <td>[delivered, highest, ratings, history, preside...</td>\n",
       "      <td>delivered highest ratings history presidential...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text  \\\n",
       "0   Neutral  RT @NancyLeeGrahn: How did everyone feel about...   \n",
       "1  Positive  RT @ScottWalker: Didn't catch the full #GOPdeb...   \n",
       "2   Neutral  RT @TJMShow: No mention of Tamir Rice and the ...   \n",
       "3  Positive  RT @RobGeorge: That Carly Fiorina is trending ...   \n",
       "4  Positive  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...   \n",
       "\n",
       "                                         text_clean1             hashtags  \\\n",
       "0       feel climate change question night gopdebate          [gopdebate]   \n",
       "1  catch full gopdebate night scotts lines second...  [gopdebate, walker]   \n",
       "2    mention tamir rice gopdebate held cleveland wow          [gopdebate]   \n",
       "3  carly fiorina trending hours debate men justco...          [gopdebate]   \n",
       "4  gopdebate delivered highest ratings history pr...   [gopdebate, trump]   \n",
       "\n",
       "                                            wordlist  \\\n",
       "0           [feel, climate, change, question, night]   \n",
       "1       [catch, full, night, scotts, lines, seconds]   \n",
       "2       [mention, tamir, rice, held, cleveland, wow]   \n",
       "3  [carly, fiorina, trending, hours, debate, men,...   \n",
       "4  [delivered, highest, ratings, history, preside...   \n",
       "\n",
       "                                   no_names_hashtags  \n",
       "0                 feel climate change question night  \n",
       "1              catch full night scotts lines seconds  \n",
       "2              mention tamir rice held cleveland wow  \n",
       "3  carly fiorina trending hours debate men justco...  \n",
       "4  delivered highest ratings history presidential...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "[    3    18   124   191   193   207   209   218   340   377   472   478\n",
      "   485   521   554   577   599   608   624   645   657   673   681   688\n",
      "   749   751   753   767   794   836   843   847   956   964   971   994\n",
      "  1000  1041  1057  1127  1133  1181  1200  1224  1266  1328  1460  1475\n",
      "  1478  1481  1552  1561  1574  1587  1621  1638  1660  1693  1713  1725\n",
      "  1741  1745  1847  1971  1984  1988  2008  2033  2102  2157  2202  2207\n",
      "  2210  2218  2247  2353  2406  2431  2470  2529  2544  2551  2651  2794\n",
      "  2807  2851  2891  2908  2937  2946  2967  2989  2998  3068  3084  3189\n",
      "  3256  3275  3307  3317  3325  3326  3417  3445  3475  3595  3631  3662\n",
      "  3678  3693  3735  3874  3939  3996  4005  4007  4024  4075  4081  4107\n",
      "  4129  4147  4181  4194  4195  4245  4251  4265  4297  4333  4335  4388\n",
      "  4415  4443  4447  4454  4658  4709  4724  4744  4747  4852  4859  4915\n",
      "  4992  4997  4999  5008  5026  5050  5063  5089  5215  5240  5294  5299\n",
      "  5368  5429  5467  5563  5580  5631  5636  5715  5743  5799  6090  6191\n",
      "  6202  6242  6273  6303  6386  6393  6413  6492  6561  6578  6645  6690\n",
      "  6768  6818  6883  6899  6954  7056  7061  7070  7097  7132  7264  7490\n",
      "  7512  7652  7673  7760  7771  7815  7866  7895  7896  7897  7900  7903\n",
      "  7911  7915  7953  7975  8086  8280  8626  8677  8685  8803  8987  9038\n",
      "  9387  9590  9598  9600 10361 10372 10495]\n",
      "fire debate performance gopdebate perry tcot\n"
     ]
    }
   ],
   "source": [
    "#Procedure that cleans \n",
    "#print(len(df[df['text_clean1'].str.contains(\"t.co\")].index.values[:]))\n",
    "print(len(df[df['text_clean1'].str.contains(re.escape('tco'))].index.values[:]))\n",
    "print(df[df['text_clean1'].str.contains(re.escape('tco'))].index.values[:])\n",
    "print(df[df['text_clean1'].str.contains(re.escape('tco'))]['text_clean1'][191])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write data, SNAPPY not available on win10 ?\n",
    "import fastparquet\n",
    "fastparquet.write('5col_DFrame.parq', df, compression='GZIP')\n",
    "#df.to_csv('5col_DFrame.csv') file size comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Continuation in .* FastText_vectors OR .*BuildModel file ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
