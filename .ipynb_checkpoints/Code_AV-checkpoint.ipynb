{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#from gensim.models.wrappers import FastText\n",
    "import gensim\n",
    "import fastparquet\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "URL = 'train_data.csv'\n",
    "def load_data(url=URL):\n",
    "    return pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10494</th>\n",
       "      <td>Negative</td>\n",
       "      <td>The question about God and the Veterans. What ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>Negative</td>\n",
       "      <td>I thought #LastComicStanding airs on Wednesday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Bingo! Put that in your article!!! #GOPDebates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @RWSurferGirl: Fox is cherry picking the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Waiting on Trumps answer about God #GOPDebates...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "10494  Negative  The question about God and the Veterans. What ...\n",
       "10495  Negative  I thought #LastComicStanding airs on Wednesday...\n",
       "10496  Negative  Bingo! Put that in your article!!! #GOPDebates...\n",
       "10497  Negative  RT @RWSurferGirl: Fox is cherry picking the ca...\n",
       "10498   Neutral  Waiting on Trumps answer about God #GOPDebates..."
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/TIE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'd', 'did', 'didn', 'do', 'does', 'doesn', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'has', 'hasn', 'have', 'haven', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'more', 'most', 'mustn', 'my', 'myself', 'needn', 'no', 'nor', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 'retweet', 'rt', 's', 'same', 'shan', 'she', 'should', 'shouldn', 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'we', 'were', 'weren', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wouldn', 'y', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "# Collecting stopwords from different sources and merging them deleting duplicates\n",
    "\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "# function to delete duplicates\n",
    "def del_dup(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "stopword_removed_sentences = []\n",
    "sw = list(stopwords.words(\"English\"))\n",
    "sw.extend([\"rt\", \"retweet\"])\n",
    "\n",
    "# # Additional stopwords from MIT\n",
    "# response = requests.get('http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop')\n",
    "# sw.extend(response.text.split())\n",
    "\n",
    "# # Deleting duplicates\n",
    "# for i in sw:\n",
    "#     sw = [ re.sub('[\\W_]+', '', x) for x in sw ]\n",
    "# sw = del_dup(sw)\n",
    "sw.remove(\"not\")\n",
    "print(sorted(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleantweet1(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(cleantweet1.pattern, '', s)\n",
    "    #words = word_tokenize(s)\n",
    "    #s = \" \".join(words)\n",
    "    s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    s = \" \".join(s)\n",
    "    return s\n",
    "\n",
    "def cleantweet2(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list of hashtags'''\n",
    "    s = re.findall(r'#(\\w+)', s)\n",
    "    s = \" \".join([word.lower() for word in s])\n",
    "    s = re.sub(r'[^a-zA-Z\\s]', '', s)\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n",
    "def cleantweet3(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : tweet without hashtags'''\n",
    "    #Read matchpattern from function property (introduced later)\n",
    "    s = re.sub(cleantweet3.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "    return s\n",
    "\n",
    "def cleantweet4(s):\n",
    "    '''\n",
    "    :s : string; a tweet\n",
    "    :return : list; words that don't contain url, @somebody, and in utf-8 and lower case\n",
    "    '''\n",
    "    s = re.sub(cleantweet4.pattern, '', s)\n",
    "    remove_punctuation_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "    # actually remove the punctutation\n",
    "    s = s.translate(remove_punctuation_map)\n",
    "    #s = \" \".join([word.lower() for word in s.split() if word.lower() not in sw])\n",
    "    #s = [word.lower() for word in s.split() if word.lower() not in sw]\n",
    "\n",
    "\n",
    "    # tokenize tweet into sentences\n",
    "    sents = sent_tokenize(s)\n",
    "    # tokenize sentences into list of words\n",
    "    words = [word_tokenize(s) for s in sents]\n",
    "    # NO IDEA\n",
    "    words = [e for sent in words for e in sent]\n",
    "    return [cleantweet4.stemmer.stem(e.lower()) for e in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleantweet1.pattern = re.compile(r'(http\\S+)')\n",
    "cleantweet3.pattern = re.compile(r'(@\\w+)|(#\\w+)|[^a-zA-Z\\s]|(\\w+:\\/\\/\\S+)', flags=re.IGNORECASE)\n",
    "#start = time.perf_counter()\n",
    "#df[\"text_clean1\"] = df.iloc[0:].text.apply(cleantweet1)\n",
    "#following is consistently faster\n",
    "df.loc[:,'text_clean1'] = df.loc[:,'text'].map(cleantweet1)\n",
    "#end = time.perf_counter()\n",
    "df.loc[:,'hashtags'] = df.loc[:,'text'].map(cleantweet2)\n",
    "#print(end - start)\n",
    "# Removing the morphological and inflexional endings from words in English.\n",
    "#cleantweet4.stemmer = PorterStemmer()\n",
    "df.loc[:,'wordlist'] = df.loc[:,'text'].map(cleantweet3)\n",
    "df.loc[:,'no_names_hashtags'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))\n",
    "#df.loc[:,'wordstring'] = df.loc[:,'wordlist'].map(lambda slocal:' '.join(slocal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean1</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>no_names_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>Positive</td>\n",
       "      <td>RT @kwrcrow: Dr. Carson remark on DC having ha...</td>\n",
       "      <td>kwrcrow dr carson remark dc half brain best li...</td>\n",
       "      <td>[gopdebates]</td>\n",
       "      <td>[dr, carson, remark, dc, half, brain, best, line]</td>\n",
       "      <td>dr carson remark dc half brain best line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>#GOPdebates bash Hillary night slick Willy is ...</td>\n",
       "      <td>gopdebates bash hillary night slick willy lovin</td>\n",
       "      <td>[gopdebates]</td>\n",
       "      <td>[bash, hillary, night, slick, willy, lovin]</td>\n",
       "      <td>bash hillary night slick willy lovin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Cancel the primaries. Fox Party set up &amp;amp; a...</td>\n",
       "      <td>cancel primaries fox party set amp anointed ma...</td>\n",
       "      <td>[gopdebates, canttrustabush, morningjoe]</td>\n",
       "      <td>[cancel, primaries, fox, party, set, amp, anoi...</td>\n",
       "      <td>cancel primaries fox party set amp anointed ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>RT @IanGaryTweets: This is real life. These pe...</td>\n",
       "      <td>iangarytweets real life people running powerfu...</td>\n",
       "      <td>[gopdebates]</td>\n",
       "      <td>[real, life, people, running, powerful, office...</td>\n",
       "      <td>real life people running powerful office world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Negative</td>\n",
       "      <td>RT @SupermanHotMale: Dear Jeb Bush, Your Recor...</td>\n",
       "      <td>supermanhotmale dear jeb bush record sir clear...</td>\n",
       "      <td>[gopdebates]</td>\n",
       "      <td>[dear, jeb, bush, record, sir, clear, fucking,...</td>\n",
       "      <td>dear jeb bush record sir clear fucking ememy v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "10000  Positive  RT @kwrcrow: Dr. Carson remark on DC having ha...   \n",
       "10001   Neutral  #GOPdebates bash Hillary night slick Willy is ...   \n",
       "10002  Negative  Cancel the primaries. Fox Party set up &amp; a...   \n",
       "10003   Neutral  RT @IanGaryTweets: This is real life. These pe...   \n",
       "10004  Negative  RT @SupermanHotMale: Dear Jeb Bush, Your Recor...   \n",
       "\n",
       "                                             text_clean1  \\\n",
       "10000  kwrcrow dr carson remark dc half brain best li...   \n",
       "10001    gopdebates bash hillary night slick willy lovin   \n",
       "10002  cancel primaries fox party set amp anointed ma...   \n",
       "10003  iangarytweets real life people running powerfu...   \n",
       "10004  supermanhotmale dear jeb bush record sir clear...   \n",
       "\n",
       "                                       hashtags  \\\n",
       "10000                              [gopdebates]   \n",
       "10001                              [gopdebates]   \n",
       "10002  [gopdebates, canttrustabush, morningjoe]   \n",
       "10003                              [gopdebates]   \n",
       "10004                              [gopdebates]   \n",
       "\n",
       "                                                wordlist  \\\n",
       "10000  [dr, carson, remark, dc, half, brain, best, line]   \n",
       "10001        [bash, hillary, night, slick, willy, lovin]   \n",
       "10002  [cancel, primaries, fox, party, set, amp, anoi...   \n",
       "10003  [real, life, people, running, powerful, office...   \n",
       "10004  [dear, jeb, bush, record, sir, clear, fucking,...   \n",
       "\n",
       "                                       no_names_hashtags  \n",
       "10000           dr carson remark dc half brain best line  \n",
       "10001               bash hillary night slick willy lovin  \n",
       "10002  cancel primaries fox party set amp anointed ma...  \n",
       "10003     real life people running powerful office world  \n",
       "10004  dear jeb bush record sir clear fucking ememy v...  "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write data, SNAPPY not available on win10 ?\n",
    "# import fastparquet\n",
    "# fastparquet.write('5col_DFrame.parq', df, compression='GZIP')\n",
    "#df.to_csv('5col_DFrame.csv') file size comparison\n",
    "\n",
    "# write data to txt\n",
    "dftrain = pd.DataFrame()\n",
    "\n",
    "dfmda = pd.DataFrame()\n",
    "\n",
    "dftrain.loc[:, 'data'] = \"__label__\" + df.loc[:,'sentiment'] + \" \" + df.loc[:,'text_clean1']\n",
    "\n",
    "import random\n",
    "\n",
    "# def some(x, n):\n",
    "#     return x.ix[random.sample(x.index, n)]\n",
    "\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "msk = np.random.rand(len(dftrain)) < 0.9\n",
    "\n",
    "train = dftrain[msk]\n",
    "\n",
    "test = dftrain[~msk]\n",
    "\n",
    "test.head()\n",
    "\n",
    "# with open('train.txt', 'w') as f:\n",
    "#     dftrain.iloc[0:10000].to_csv(f, header=None, index=None, sep='\\n', quoting=csv.QUOTE_NONE, quotechar='')\n",
    "\n",
    "\n",
    "# with open('test.txt', 'w') as f:\n",
    "#     dftrain.iloc[10000:].to_csv(f, header=None, index=None, sep='\\n', quoting=csv.QUOTE_NONE, quotechar='')\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    train.to_csv(f, header=None, index=None, sep='\\n', quoting=csv.QUOTE_NONE, quotechar='')\n",
    "\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "    test.to_csv(f, header=None, index=None, sep='\\n', quoting=csv.QUOTE_NONE, quotechar='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###FAST TEXT###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "classifier = fasttext.supervised('train.txt', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.6666666666666666\n",
      "R@1: 0.6666666666666666\n",
      "Number of examples: 1065\n"
     ]
    }
   ],
   "source": [
    "result = classifier.test('test.txt')\n",
    "\n",
    "\n",
    "print ('P@1:', result.precision)\n",
    "print ('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Negative'], ['Positive']]\n"
     ]
    }
   ],
   "source": [
    "texts = ['motherfucking', 'I really like what senator mccain said about the future of our great nation']\n",
    "labels = classifier.predict(texts)\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
